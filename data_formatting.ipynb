{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Before this, you should have downloaded and unzipped all the data from kaggle.\n",
    "Be sure to update your working directory to get the data all formatted.\n",
    "\n",
    "This is directly from https://www.kaggle.com/yhn112/resnet18-baseline-pytorch-ignite/notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_6356/3075135867.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mPIL\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mImage\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mD\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Be sure to update path_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x180b9ce3310>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data = 'D:\\\\Data\\\\DeepLearning\\\\recursion-cellular-image-classification'\n",
    "batch_size = 32\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "class ImagesDS(D.Dataset):\n",
    "    def __init__(self, df, img_dir, mode='train', channels=[1,2,3,4,5,6], transform=None):\n",
    "        self.records = df.to_records(index=False)\n",
    "        self.channels = channels\n",
    "        # self.site = site\n",
    "        self.mode = mode\n",
    "        self.img_dir = img_dir\n",
    "        self.len = df.shape[0]\n",
    "        self.transform = transform\n",
    "        unique_list = np.unique([int(n.split(\"_\")[1]) for n in df['sirna']])\n",
    "        self.mapping = {}\n",
    "        for (i, val) in enumerate(unique_list):\n",
    "            self.mapping[val] = i\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_img_as_tensor(file_name):\n",
    "        with Image.open(file_name) as img:\n",
    "            return T.ToTensor()(img)\n",
    "\n",
    "    def _get_img_path(self, index, channel, site):\n",
    "        mode = self.mode\n",
    "        if self.mode == 'valid':\n",
    "            mode = 'train'\n",
    "        experiment, well, plate = self.records[index].experiment, self.records[index].well, self.records[index].plate\n",
    "        return '/'.join([self.img_dir,mode,experiment,f'Plate{plate}',f'{well}_s{site}_w{channel}.png'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        paths_1 = [self._get_img_path(index, ch, 1) for ch in self.channels]\n",
    "        paths_2 = [self._get_img_path(index, ch, 2) for ch in self.channels]\n",
    "        img_1 = torch.cat([self._load_img_as_tensor(img_path) for img_path in paths_1])\n",
    "        img_2 = torch.cat([self._load_img_as_tensor(img_path) for img_path in paths_2])\n",
    "        img = torch.cat([img_1, img_2])\n",
    "        if self.mode == 'train':\n",
    "            # if training, then apply transformation\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, self.mapping[int(self.records[index].sirna.split(\"_\")[1])]\n",
    "        elif self.mode == 'valid':\n",
    "            return img, self.mapping[int(self.records[index].sirna.split('_')[1])]\n",
    "        else:\n",
    "            return img, self.records[index].id_code\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define transformations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(90),\n",
    "    T.RandomVerticalFlip()\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gets data from site 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_data+'/train.csv')\n",
    "df_train, df_test = train_test_split(df, test_size = 0.1, random_state=42)\n",
    "df_train, df_val = train_test_split(df_train, test_size=.01, random_state=42)\n",
    "# df_test = pd.read_csv(path_data+'/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "ds_train = ImagesDS(df_train, path_data, mode='train', transform=transforms)\n",
    "ds_val = ImagesDS(df_val, path_data, mode='valid')\n",
    "ds_test = ImagesDS(df_test, path_data, mode='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creates DataLoader which is what is used for the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "train_loader = D.DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = D.DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = D.DataLoader(ds_val, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example model training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "\n",
    "classes = 1108\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, classes)\n",
    "\n",
    "trained_kernel = model.conv1.weight\n",
    "new_conv = nn.Conv2d(12, 64, 7, 2, 3, bias=False)\n",
    "with torch.no_grad():\n",
    "    new_conv.weight[:,:] = torch.stack([torch.mean(trained_kernel, 1)] * 12, dim=1)\n",
    "model.conv1 = new_conv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def validation_accuracy(model, testloader, p=False):\n",
    "    with torch.no_grad(): # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in testloader:\n",
    "\n",
    "            '''Step - Move images to device after appropriate reshaping'''\n",
    "            images = images.to(device)\n",
    "            '''Step  - Move labels to device'''\n",
    "            labels = labels.to(device)\n",
    "            #get network outputs\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        if p:\n",
    "            print(f'Accuracy of the network on the 10000 test images: {(100 * correct / total)}')\n",
    "        return 100 * correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<built-in method item of Tensor object at 0x0000018082B539F0>\n",
      "1\n",
      "<built-in method item of Tensor object at 0x0000018082B484A0>\n",
      "2\n",
      "<built-in method item of Tensor object at 0x0000018082B483B0>\n",
      "3\n",
      "<built-in method item of Tensor object at 0x0000018082B48C20>\n",
      "4\n",
      "<built-in method item of Tensor object at 0x0000018082B489F0>\n",
      "5\n",
      "<built-in method item of Tensor object at 0x0000018082B48900>\n",
      "6\n",
      "<built-in method item of Tensor object at 0x0000018082B480E0>\n",
      "7\n",
      "<built-in method item of Tensor object at 0x0000018082B48860>\n",
      "8\n",
      "<built-in method item of Tensor object at 0x0000018082B48D10>\n",
      "9\n",
      "<built-in method item of Tensor object at 0x0000018082B486D0>\n",
      "Epoch [1/1], Step [10/925], Loss: 7.4407\n",
      "10\n",
      "<built-in method item of Tensor object at 0x0000018082B489F0>\n",
      "11\n",
      "<built-in method item of Tensor object at 0x0000018082B484F0>\n",
      "12\n",
      "<built-in method item of Tensor object at 0x0000018082B48040>\n",
      "13\n",
      "<built-in method item of Tensor object at 0x0000018082B48C20>\n",
      "14\n",
      "<built-in method item of Tensor object at 0x0000018082B48310>\n",
      "15\n",
      "<built-in method item of Tensor object at 0x0000018082B48CC0>\n",
      "16\n",
      "<built-in method item of Tensor object at 0x0000018082B487C0>\n",
      "17\n",
      "<built-in method item of Tensor object at 0x0000018082B48D10>\n",
      "18\n",
      "<built-in method item of Tensor object at 0x0000018082B48C20>\n",
      "19\n",
      "<built-in method item of Tensor object at 0x0000018082B48DB0>\n",
      "Epoch [1/1], Step [20/925], Loss: 7.5982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Nepochs = 1\n",
    "Nbatch = 20\n",
    "\n",
    "stop = False\n",
    "minimum_loss = .8\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "for i in range(Nepochs):\n",
    "    if stop:\n",
    "        break\n",
    "    for j, batch in enumerate(train_loader):\n",
    "        print(j)\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss.item())\n",
    "        if (j+1) % 10 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(i+1, Nepochs, j+1, len(train_loader), loss.item()))\n",
    "\n",
    "        Nbatch -= 1\n",
    "        if Nbatch <= 0 or loss <= minimum_loss:\n",
    "            stop = True\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 0.303951367781155\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.303951367781155"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_accuracy(model, valid_loader, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}